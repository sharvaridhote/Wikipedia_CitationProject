{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Evaluating-three-trained-models-with-two-test-dataset\" data-toc-modified-id=\"Evaluating-three-trained-models-with-two-test-dataset-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Evaluating three trained models with two test dataset</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>Model</a></span></li><li><span><a href=\"#Test-Dataset\" data-toc-modified-id=\"Test-Dataset-1.0.2\"><span class=\"toc-item-num\">1.0.2&nbsp;&nbsp;</span>Test Dataset</a></span></li></ul></li></ul></li><li><span><a href=\"#Explaining-the-classifier-prediction-using-LIME\" data-toc-modified-id=\"Explaining-the-classifier-prediction-using-LIME-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Explaining the classifier prediction using LIME</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating three trained models with two test dataset\n",
    "####  Model\n",
    " - Model 1 -  trained using sentences feature article train dataset which includes context.\n",
    " - Model 2 -  trained using Wikipedia open feature article dataset which included context\n",
    "\n",
    "\n",
    "#### Test Dataset\n",
    " - test dataset 1 : Random Wikipedia dataset\n",
    " - test dataset 2 : Citation needed Wikipedia dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing model with two different test datasets \n",
    "# Random articles - biography topic\n",
    "# citation needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import random\n",
    "import spacy\n",
    "from util import df_tolist, label_cat\n",
    "from train import training, evaluate, load_data\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Sharpest_Mind\\WikipediaCitation\\src\\Notebooks\\util.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tuples'] = df.apply(lambda row: (row[df.columns[0]], row[df.columns[1]]), axis=1)\n"
     ]
    }
   ],
   "source": [
    "df4 = pd.read_csv('E:/Sharpest_Mind/WikipediaCitation/data/processed/csv_files/sort/Citation_needed_wiki.csv', encoding = 'ISO-8859-1')\n",
    "df4 = shuffle(df4)\n",
    "df4_1 = df4[df4['label']==1] \n",
    "df4_0 = df4[df4['label']==0]\n",
    "df4_test = pd.concat([df4_1[:2000],df4_0[:2000]])\n",
    "test1 =df_tolist(df4_test)\n",
    "test_texts1, test_cats1 = zip(*test1)\n",
    "test_cats1 = [{\"POSITIVE\": bool(y), \"NEGATIVE\": not bool(y)} for y in test_cats1]\n",
    "test_data1 = list(zip(test_texts1,[{'cats': cats} for cats in test_cats1]))\n",
    "print(len(test_data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3980\n"
     ]
    }
   ],
   "source": [
    "# Converting the dataframe into a list of tuples\n",
    "# wikidata random articles -biography section\n",
    "df5 = pd.read_csv('E:/Sharpest_Mind/WikipediaCitation/data/processed/csv_files/sort/wiki_data_test.csv', encoding = 'ISO-8859-1')\n",
    "df5 = shuffle(df5)\n",
    "df5_1 = df5[df5['Label']==1] \n",
    "df5_0 = df5[df5['Label']==0]\n",
    "df5_test = pd.concat([df5_1[8000:],df5_0[8000:]])\n",
    "test2 =df_tolist(df5_test)\n",
    "\n",
    "random.shuffle(test2)\n",
    "\n",
    "test_texts2, test_cats2 = zip(*test2)\n",
    "test_cats2 = [{\"POSITIVE\": bool(y), \"NEGATIVE\": not bool(y)} for y in test_cats2]\n",
    "test_data2 = list(zip(test_texts2,[{'cats': cats} for cats in test_cats2]))\n",
    "print(len(test_data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random article test data :  {'textcat_a': 0.9383999999962465, 'textcat_p': 0.9013303769368022, 'textcat_r': 0.9834677419315183, 'textcat_f': 0.940609332815501}\n",
      "Citation needed test data :  {'textcat_a': 0.5299999999978801, 'textcat_p': 0.9999999999962265, 'textcat_r': 0.52999999999894, 'textcat_f': 0.6928104575145286}\n"
     ]
    }
   ],
   "source": [
    "# Load spacy model based on statements context\n",
    "loaded_model1 = spacy.load('E:/Sharpest_Mind/WikipediaCitation/src/Notebooks/model_artifactnewdatatest1-2e-3D')\n",
    "textcat1 = loaded_model1.get_pipe('textcat') \n",
    "\n",
    "# Evaluate results from my statements data test with random article\n",
    "scores1= evaluate(loaded_model1, textcat1, test_texts1, test_cats1)\n",
    "print('Random article test data : ', scores1)\n",
    "\n",
    "# Evaluate results from my statements data test with citation needed article\n",
    "scores2= evaluate(loaded_model1, textcat1, test_texts2, test_cats2)\n",
    "print('Citation needed test data : ', scores2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spacy model based on statements context\n",
    "loaded_model11 = spacy.load('E:/Sharpest_Mind/WikipediaCitation/src/Notebooks/model_artifactnewdatatest1-2e-3D')\n",
    "textcat11 = loaded_model11.get_pipe('textcat') \n",
    "\n",
    "# Evaluate results from my statements data test with random article\n",
    "scores11= evaluate(loaded_model11, textcat1, test_texts1, test_cats1)\n",
    "print('Random article test data : ', scores1)\n",
    "\n",
    "# Evaluate results from my statements data test with citation needed article\n",
    "scores21= evaluate(loaded_model11, textcat1, test_texts2, test_cats2)\n",
    "print('Citation needed test data : ', scores2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random article test data :  {'textcat_a': 0.9395999999962417, 'textcat_p': 0.9054355919549314, 'textcat_r': 0.9806451612863684, 'textcat_f': 0.9415408439762233}\n",
      "Citation needed test data :  {'textcat_a': 0.5295999999978817, 'textcat_p': 0.9999999999962236, 'textcat_r': 0.5295999999989408, 'textcat_f': 0.6924686192450511}\n"
     ]
    }
   ],
   "source": [
    "# statements traindata model with statements - test additional wiki citation needed sentences\n",
    "loaded_model2 = spacy.load('E:/Sharpest_Mind/WikipediaCitation/notebooks/model_artifactnewdataLensemble')\n",
    "textcat2 = loaded_model2.get_pipe('textcat') \n",
    "\n",
    "# Evaluate results from my statements data test with random article\n",
    "scores3= evaluate(loaded_model2, textcat2, test_texts1, test_cats1)\n",
    "print('Random article test data : ', scores3)\n",
    "\n",
    "# Evaluate results from Wikipedia sentences data test with citation needed article\n",
    "scores4 = evaluate(loaded_model2, textcat2, test_texts2, test_cats2)\n",
    "print('Citation needed test data : ', scores4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random article test data :  {'textcat_a': 0.9999999900000002, 'textcat_p': 0.9999999900000002, 'textcat_r': 0.9999999900000002, 'textcat_f': 0.9999999900000002}\n",
      "Citation needed test data :  {'textcat_a': 0.9999999900000002, 'textcat_p': 0.999999995, 'textcat_r': 0.999999995, 'textcat_f': 0.999999995}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# Load spacy model based on statements context\n",
    "loaded_model3 = spacy.load('E:/Sharpest_Mind/WikipediaCitation/src/Models/model_artifactnWikidataL2-2e-4')\n",
    "\n",
    "textcat3 = loaded_model3.get_pipe('textcat') \n",
    "\n",
    "# Evaluate results from my statements data test with random article\n",
    "scores5= evaluate(loaded_model3, textcat3, test_texts1[:2], test_cats1[:2])\n",
    "print('Random article test data : ', scores5)\n",
    "\n",
    "# Evaluate results from my statements data test with citation needed article\n",
    "scores6 = evaluate(loaded_model3, textcat3, test_texts2[:2], test_cats2[:2])\n",
    "print('Citation needed test data : ', scores6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random article test data :  {'textcat_a': 0.542999999997828, 'textcat_p': 0.6431718061579563, 'textcat_r': 0.17661290322509432, 'textcat_f': 0.2771274912984681}\n",
      "Citation needed test data :  {'textcat_a': 0.20859999999916562, 'textcat_p': 0.9999999999904123, 'textcat_r': 0.2085999999995828, 'textcat_f': 0.3451927850394003}\n"
     ]
    }
   ],
   "source": [
    "# Load spacy model based on statements context\n",
    "loaded_model4 = spacy.load('E:/Sharpest_Mind/WikipediaCitation/notebooks/model_artifactnewdataL2-0.001')\n",
    "\n",
    "textcat4 = loaded_model4.get_pipe('textcat') \n",
    "\n",
    "# Evaluate results from my statements data test with random article\n",
    "scores7= evaluate(loaded_model4, textcat4, test_texts1, test_cats1)\n",
    "print('Random article test data : ', scores7)\n",
    "\n",
    "# Evaluate results from my statements data test with citation needed article\n",
    "scores8 = evaluate(loaded_model4, textcat4, test_texts2, test_cats2)\n",
    "print('Citation needed test data : ', scores8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textcat_a</th>\n",
       "      <th>textcat_p</th>\n",
       "      <th>textcat_r</th>\n",
       "      <th>textcat_f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>modelwiki1</th>\n",
       "      <td>0.9384</td>\n",
       "      <td>0.901330</td>\n",
       "      <td>0.983468</td>\n",
       "      <td>0.940609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modelwiki2</th>\n",
       "      <td>0.5300</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>0.692810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modelwiki3</th>\n",
       "      <td>0.9396</td>\n",
       "      <td>0.905436</td>\n",
       "      <td>0.980645</td>\n",
       "      <td>0.941541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modelnewdata4</th>\n",
       "      <td>0.5296</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.529600</td>\n",
       "      <td>0.692469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modelnewdata5</th>\n",
       "      <td>0.5820</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.241935</td>\n",
       "      <td>0.364742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modelnewdata6</th>\n",
       "      <td>0.2564</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.256400</td>\n",
       "      <td>0.408150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modelnewdata7</th>\n",
       "      <td>0.5430</td>\n",
       "      <td>0.643172</td>\n",
       "      <td>0.176613</td>\n",
       "      <td>0.277127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modelnewdata8</th>\n",
       "      <td>0.2086</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.208600</td>\n",
       "      <td>0.345193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               textcat_a  textcat_p  textcat_r  textcat_f\n",
       "modelwiki1        0.9384   0.901330   0.983468   0.940609\n",
       "modelwiki2        0.5300   1.000000   0.530000   0.692810\n",
       "modelwiki3        0.9396   0.905436   0.980645   0.941541\n",
       "modelnewdata4     0.5296   1.000000   0.529600   0.692469\n",
       "modelnewdata5     0.5820   0.740741   0.241935   0.364742\n",
       "modelnewdata6     0.2564   1.000000   0.256400   0.408150\n",
       "modelnewdata7     0.5430   0.643172   0.176613   0.277127\n",
       "modelnewdata8     0.2086   1.000000   0.208600   0.345193"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results =  pd.DataFrame.from_dict([scores1, scores2, scores3, scores4, scores5, scores6, scores7, scores8])\n",
    "df_results.index = ['modelwiki1', 'modelwiki2', 'modelwiki3','modelnewdata4', 'modelnewdata5', 'modelnewdata6', 'modelnewdata7', 'modelnewdata8']\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Explaining the classifier prediction using LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import click\n",
    "import streamlit.cli\n",
    "\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from spacy.util import load_model_from_path\n",
    "from os.path import join, dirname, abspath\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'POSITIVE': 6.328377266662244e-10, 'NEGATIVE': 1.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Negative instance hhttps://en.wikipedia.org/wiki/1983_Burlington_mayoral_election\n",
    "test_text = 'The 1983 Burlington mayoral election was held March 1, 1983., Paul Giraud, Augustus Koch, D. D. Morse, Henry Welge, and A. L. Westyard.'\n",
    "doc=loaded_model1(test_text)\n",
    "doc.cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'POSITIVE': 0.994175910949707, 'NEGATIVE': 0.005824094172567129}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Positive instance hhttps://en.wikipedia.org/wiki/1983_Burlington_mayoral_election\n",
    "test_text = ' Incumbent Mayor Bernie Sanders won with 52.12% of the popular vote against Democratic nominee Judith Stephany and Republican nominee James Gilson.'\n",
    "doc=loaded_model1(test_text)\n",
    "doc.cats\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'POSITIVE': 1.0572063047220581e-07, 'NEGATIVE': 0.9999998807907104}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Positive instance https://en.wikipedia.org/wiki/Pictorial_map\n",
    "test_text = 'The leading panoramic map artists in the U.S.A. were Herman Brosius, Camille N. Drie, Thaddeus Mortimer Fowler, Paul Giraud, Augustus Koch, D. D. Morse, Henry Welge, and A. L. Westyard.'\n",
    "doc=loaded_model1(test_text)\n",
    "doc.cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'POSITIVE': 0.002851887373253703, 'NEGATIVE': 0.9971480965614319}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Negative  instance https://en.wikipedia.org/wiki/Pictorial_map\n",
    "test_text = 'As cartography evolved, the pictorial art form went its own way and regained popularity in the 19th century with the development of the railroads. Between 1825 and 1875, the production and collection of panoramic maps of cities rose to something of a mania. '\n",
    "doc=loaded_model1(test_text)\n",
    "doc.cats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Load model\n",
    "loaded_model1 = spacy.load('E:/Sharpest_Mind/WikipediaCitation/notebooks/model_artifactwikitest2-2e-3')\n",
    "textcat1 = loaded_model1.get_pipe('textcat')\n",
    "\n",
    "# Load airline CSV\n",
    "df1 = pd.read_csv('E:/Sharpest_Mind/WikipediaCitation/notebooks/wiki_data.csv', encoding = 'utf8')\n",
    "\n",
    "df1['Statements'] = (df1['Statements'].map(lambda x: unicodedata.normalize('NFKD', str(x))))\n",
    "\n",
    "train_df = df1[['Statements', 'label']]\n",
    "cats = list(train_df['label'].unique())\n",
    "\n",
    "arr_len = len(cats)\n",
    "\n",
    "# Initialize LIME text explainer\n",
    "explainer = LimeTextExplainer(class_names=cats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_prob(context):\n",
    "    \"\"\"\n",
    "    Provideds sklearn style predict_proba output\n",
    "    Params:\n",
    "    -------\n",
    "    context: str - text content to evaluate\n",
    "    Returns:\n",
    "    --------\n",
    "    numpy array of probabilities\n",
    "    \"\"\"\n",
    "\n",
    "    fin_arr = np.zeros(shape=(1, arr_len))\n",
    "   \n",
    "\n",
    "    for txt in context:\n",
    "        loaded_model1 = spacy.load('E:/Sharpest_Mind/WikipediaCitation/models/final_model/Modeldatawiki')\n",
    "        textcat1 = loaded_model1.get_pipe('textcat')\n",
    "        doc = loaded_model1(txt)\n",
    "        preds = np.array(list(doc.cats.values())).reshape(1, arr_len)\n",
    "        fin_arr = np.vstack((fin_arr, preds))\n",
    "\n",
    "    return fin_arr[1:]\n",
    "\n",
    "\n",
    "def explain_text_features(context, viz_features):\n",
    "    \"\"\"\n",
    "    Explains the mot important features of text\n",
    "    Params:\n",
    "    -------\n",
    "    context: str - text content to evaluate\n",
    "    viz_features: int - Number of features to visualize. Defaults to 5\n",
    "    Returns:\n",
    "    --------\n",
    "    Explanation of features for class\n",
    "    \"\"\"\n",
    "\n",
    "    # Print predicted class\n",
    "   # Print predicted class\n",
    "    doc = loaded_model1(context, delete=delete_patterns)\n",
    "    outcome = max(doc.cats, key=doc.cats.get)\n",
    "    # print(f'Predicted class - {outcome}\\n')\n",
    "    # print(f'Predicted class - {outcome}\\n')\n",
    "\n",
    "    # Initialize explainer\n",
    "    exp = explainer.explain_instance(context, predict_prob, labels=[0, 1],\n",
    "                                     num_features=viz_features)\n",
    "\n",
    "\n",
    "    # Initialize a matpotlib figure\n",
    "    fig = plt.figure(figsize=(5, 10))\n",
    "\n",
    "    # Create subplot for Negative Class\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.title('Features contribution for class Negative', fontsize=21,\n",
    "              color='blue')\n",
    "    neu_values = [i[1] for i in exp.as_list(label=0)]\n",
    "    neu_words = [i[0] for i in exp.as_list(label=0)]\n",
    "    neu_colors = ['green' if x > 0 else 'red' for x in neu_values]\n",
    "    plt.barh(y=neu_words, width=neu_values, align='center', color=neu_colors)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "\n",
    "    # Create subplot for Positive Class\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.title('Features contribution for class Positive', fontsize=21,\n",
    "              color='blue')\n",
    "    pos_values = [i[1] for i in exp.as_list(label=1)]\n",
    "    pos_words = [i[0] for i in exp.as_list(label=1)]\n",
    "    pos_colors = ['green' if x > 0 else 'red' for x in pos_values]\n",
    "    plt.barh(y=pos_words, width=pos_values, align='center', color=pos_colors)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "\n",
    "    \n",
    "\n",
    "    # Create a Super Title\n",
    "    plt.suptitle('Classwise most important features', fontsize=34,\n",
    "                 color='navy')\n",
    "\n",
    "    return outcome, fig, exp.as_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'POSITIVE': 0.9959408044815063, 'NEGATIVE': 0.004059239290654659}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the model = Positive instance \n",
    "test_text= \"Such positions also allowed him to play a central role in training the militia forces who repelled the Bay of Pigs Invasion and bringing the Soviet nuclear-armed ballistic missiles to Cuba which precipitated the 1962 Cuban Missile Crisis \"\n",
    "doc=loaded_model1(test_text)\n",
    "doc.cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't read file: E:\\Sharpest_Mind\\WikipediaCitation\\models\\final_model\\Modeldatawiki\\tagger\\tag_map",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-9970f219174a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#context = 'Later, during the Age of Exploration, maps became progressively more accurate for navigation needs and were often sprinkled with sketches and drawings such as sailing ships showing the direction of trade winds, little trees and mounds to represent forests and mountains and of course, plenty of sea creatures and exotic natives much of them imaginary. As the need for geographical accuracy increased, these illustrations gradually slipped off the map and onto the borders and eventually disappeared altogether in the wake of modern scientific cartography.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'Such positions also allowed him to play a central role in training the militia forces who repelled the Bay of Pigs Invasion and bringing the Soviet nuclear-armed ballistic missiles to Cuba which precipitated the 1962 Cuban Missile Crisis'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfeature_ex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexplain_text_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-30-1390c4d50476>\u001b[0m in \u001b[0;36mexplain_text_features\u001b[1;34m(context, viz_features)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;31m# Initialize explainer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     exp = explainer.explain_instance(context, predict_prob, labels=[0, 1],\n\u001b[1;32m---> 44\u001b[1;33m                                      num_features=viz_features)\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lime\\lime_text.py\u001b[0m in \u001b[0;36mexplain_instance\u001b[1;34m(self, text_instance, classifier_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[0;32m    413\u001b[0m         data, yss, distances = self.__data_labels_distances(\n\u001b[0;32m    414\u001b[0m             \u001b[0mindexed_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m             distance_metric=distance_metric)\n\u001b[0m\u001b[0;32m    416\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_names\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lime\\lime_text.py\u001b[0m in \u001b[0;36m__data_labels_distances\u001b[1;34m(self, indexed_string, classifier_fn, num_samples, distance_metric)\u001b[0m\n\u001b[0;32m    480\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minactive\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m             \u001b[0minverse_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexed_string\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_removing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minactive\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minverse_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistance_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-1390c4d50476>\u001b[0m in \u001b[0;36mpredict_prob\u001b[1;34m(context)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtxt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mloaded_model1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'E:/Sharpest_Mind/WikipediaCitation/models/final_model/Modeldatawiki'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mtextcat1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_model1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_pipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'textcat'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_model1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_package\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# path to model data directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"exists\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Path or Path-like to model data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[1;34m(model_path, meta, **overrides)\u001b[0m\n\u001b[0;32m    220\u001b[0m             \u001b[0mcomponent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_pipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfactory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m             \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[1;34m(self, path, exclude, disable)\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;31m# Convert to list here in case exclude is (default) tuple\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    973\u001b[0m             \u001b[0mexclude\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"vocab\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 974\u001b[1;33m         \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeserializers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    975\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[1;34m(path, readers, exclude)\u001b[0m\n\u001b[0;32m    688\u001b[0m         \u001b[1;31m# Split to support file names like meta.json\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 690\u001b[1;33m             \u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    691\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(p, proc)\u001b[0m\n\u001b[0;32m    967\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m             deserializers[name] = lambda p, proc=proc: proc.from_disk(\n\u001b[1;32m--> 969\u001b[1;33m                 \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"vocab\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    970\u001b[0m             )\n\u001b[0;32m    971\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m\"vocab\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"vocab\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpipes.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.pipes.Tagger.from_disk\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[1;34m(path, readers, exclude)\u001b[0m\n\u001b[0;32m    688\u001b[0m         \u001b[1;31m# Split to support file names like meta.json\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 690\u001b[1;33m             \u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    691\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpipes.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.pipes.Tagger.from_disk.load_tag_map\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\srsly\\_msgpack_api.py\u001b[0m in \u001b[0;36mread_msgpack\u001b[1;34m(location, use_list)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mRETURNS\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdeserialized\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \"\"\"\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[0mfile_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforce_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;31m# msgpack-python docs suggest disabling gc before unpacking large messages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\srsly\\util.py\u001b[0m in \u001b[0;36mforce_path\u001b[1;34m(location, require_exists)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mlocation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mrequire_exists\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Can't read file: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Can't read file: E:\\Sharpest_Mind\\WikipediaCitation\\models\\final_model\\Modeldatawiki\\tagger\\tag_map"
     ]
    }
   ],
   "source": [
    "#context = 'Later, during the Age of Exploration, maps became progressively more accurate for navigation needs and were often sprinkled with sketches and drawings such as sailing ships showing the direction of trade winds, little trees and mounds to represent forests and mountains and of course, plenty of sea creatures and exotic natives much of them imaginary. As the need for geographical accuracy increased, these illustrations gradually slipped off the map and onto the borders and eventually disappeared altogether in the wake of modern scientific cartography.'\n",
    "context ='Such positions also allowed him to play a central role in training the militia forces who repelled the Bay of Pigs Invasion and bringing the Soviet nuclear-armed ballistic missiles to Cuba which precipitated the 1962 Cuban Missile Crisis'\n",
    "feature_ex = explain_text_features(context, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model Negative instance\n",
    "test_text=\"Later, during the Age of Exploration, maps became progressively more accurate for navigation needs and were often sprinkled with sketches and drawings such as sailing ships showing the direction of trade winds, little trees and mounds to represent forests and mountains and of course, plenty of sea creatures and exotic natives much of them imaginary. As the need for geographical accuracy increased, these illustrations gradually slipped off the map and onto the borders and eventually disappeared altogether in the wake of modern scientific cartography.\"\n",
    "doc=loaded_model1(test_text)\n",
    "doc.cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = 'Later, during the Age of Exploration, maps became progressively more accurate for navigation needs and were often sprinkled with sketches and drawings such as sailing ships showing the direction of trade winds, little trees and mounds to represent forests and mountains and of course, plenty of sea creatures and exotic natives much of them imaginary. As the need for geographical accuracy increased, these illustrations gradually slipped off the map and onto the borders and eventually disappeared altogether in the wake of modern scientific cartography.'\n",
    "#context ='The leading panoramic map artists in the U.S.A. were Herman Brosius, Camille Drie, Thaddeus Mortimer Fowler'\n",
    "feature_ex = explain_text_features(context, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
